name: Frontend CI/CD Pipeline

on:
  push:
    branches: [main, develop, feature/*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy"
        required: true
        default: "staging"
        type: choice
        options:
          - staging
          - production
      skip_tests:
        description: "Skip test execution"
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: "20"
  AWS_REGION: us-east-1
  S3_BUCKET_STAGING: shrt-frontend-staging
  S3_BUCKET_PRODUCTION: shrt-frontend-production

jobs:
  # Job para an√°lisis de c√≥digo y tests
  quality-checks:
    name: Code Quality & Tests
    runs-on: ubuntu-latest
    if: ${{ !inputs.skip_tests }}

    outputs:
      cache-key: ${{ steps.cache.outputs.cache-hit }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Cache Dependencies
        id: cache
        uses: actions/cache@v3
        with:
          path: |
            node_modules
            ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: Install Dependencies
        run: |
          # Always use npm install to handle lock file sync issues
          npm install --prefer-offline --no-audit --legacy-peer-deps

      - name: Lint Code
        run: npm run lint

      - name: Type Check
        run: npm run typecheck

      - name: Run Unit Tests
        run: npm test -- --coverage --watchAll=false --maxWorkers=2

      - name: Upload Coverage Reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info
          flags: frontend
          name: frontend-coverage
          fail_ci_if_error: false

      - name: Upload package-lock.json
        uses: actions/upload-artifact@v4
        with:
          name: package-lock-${{ github.sha }}
          path: package-lock.json
          retention-days: 1
        if: hashFiles('package-lock.json') != ''

      - name: SonarCloud Analysis
        uses: SonarSource/sonarcloud-github-action@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        continue-on-error: true

  # Job para construir la aplicaci√≥n
  build:
    name: Build Application via Docker
    runs-on: ubuntu-latest
    needs: quality-checks
    if: always() && (needs.quality-checks.result == 'success' || inputs.skip_tests)

    strategy:
      matrix:
        environment: [staging, production]
        include:
          - environment: staging
            api_url_secret: VITE_API_URL_STAGING
            app_url_secret: VITE_APP_URL_STAGING
            cloudfront_domain_secret: CLOUDFRONT_DOMAIN_STAGING
            enable_debug: true
            enable_monitoring: false
            show_performance_metrics: true
            deploy_condition: github.ref == 'refs/heads/develop'
          - environment: production
            api_url_secret: VITE_API_URL_PRODUCTION
            app_url_secret: VITE_APP_URL_PRODUCTION
            cloudfront_domain_secret: CLOUDFRONT_DOMAIN_PRODUCTION
            enable_debug: false
            enable_monitoring: true
            show_performance_metrics: false
            deploy_condition: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Build Docker Image
        id: docker_build
        run: |
          IMAGE_TAG="frontend-${{ matrix.environment }}:${{ github.sha }}"
          echo "Building Docker image: $IMAGE_TAG"

          # Debug: Show what secrets we're using
          echo "üîç Environment variables that will be injected:"
          echo "VITE_API_URL: ${{ secrets[matrix.api_url_secret] }}"
          echo "VITE_APP_URL: ${{ secrets[matrix.app_url_secret] }}"
          echo "VITE_APP_ENV: ${{ matrix.environment }}"

          # Build Docker image with direct args (avoid complex variable handling)
          BUILD_TIME="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          # Build with environment-specific values from secrets
          docker build . -f Dockerfile.prod -t "$IMAGE_TAG" \
            --build-arg "VITE_API_URL=${{ secrets[matrix.api_url_secret] }}" \
            --build-arg "VITE_API_BASE_URL=/api" \
            --build-arg "VITE_APP_URL=${{ secrets[matrix.app_url_secret] }}" \
            --build-arg "VITE_APP_ENV=${{ matrix.environment }}" \
            --build-arg "VITE_APP_NAME=Shrt" \
            --build-arg "VITE_APP_VERSION=1.0.0" \
            --build-arg "VITE_COMMIT_HASH=${{ github.sha }}" \
            --build-arg "VITE_COMMIT_MESSAGE=Deploy from ${{ github.ref_name }}" \
            --build-arg "VITE_GIT_BRANCH=${{ github.ref_name }}" \
            --build-arg "VITE_BUILD_TIME=${BUILD_TIME}" \
            --build-arg "VITE_REQUEST_TIMEOUT=5000" \
            --build-arg "VITE_ENABLE_DEBUG=${{ matrix.enable_debug }}" \
            --build-arg "VITE_ENABLE_MONITORING=${{ matrix.enable_monitoring }}" \
            --build-arg "VITE_SHOW_PERFORMANCE_METRICS=${{ matrix.show_performance_metrics }}"
          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Extract 'dist' from Docker Image
        run: |
          echo "Extracting build artifacts from image ${{ steps.docker_build.outputs.image_tag }}"
          mkdir -p ./dist
          CONTAINER_ID=$(docker create ${{ steps.docker_build.outputs.image_tag }})
          docker cp $CONTAINER_ID:/usr/share/nginx/html/. ./dist
          docker rm $CONTAINER_ID

      - name: Verify Build Output
        run: |
          echo "üîç Verifying extracted build output..."
          ls -la ./dist
          if [ -f "dist/index.html" ]; then
            echo "‚úÖ index.html exists in extracted artifacts."
            echo "üîç Checking build structure..."

            # Check if JavaScript files exist
            JS_FILES=$(find dist/ -name "*.js" | wc -l)
            if [ "$JS_FILES" -gt 0 ]; then
              echo "‚úÖ Found $JS_FILES JavaScript file(s)"
              find dist/ -name "*.js" -exec basename {} \; | sort
            else
              echo "‚ùå No JavaScript files found"
              exit 1
            fi

            # Check if CSS files exist
            CSS_FILES=$(find dist/ -name "*.css" | wc -l)
            if [ "$CSS_FILES" -gt 0 ]; then
              echo "‚úÖ Found $CSS_FILES CSS file(s)"
            else
              echo "‚ö†Ô∏è No CSS files found (this might be okay)"
            fi

            # Check index.html structure
            echo "üîç Checking index.html structure..."
            if grep -q "<script" dist/index.html; then
              echo "‚úÖ index.html contains script tags"
            else
              echo "‚ùå index.html missing script tags"
              exit 1
            fi

            if grep -q "<div id=\"root\"" dist/index.html; then
              echo "‚úÖ index.html contains React root element"
            else
              echo "‚ùå index.html missing React root element"
              exit 1
            fi

            echo "‚úÖ BUILD VERIFICATION PASSED"
            echo "üì¶ Using build-time environment variable injection"
            echo "üîß Variables are embedded directly in the static build"

            # Test the Docker image
            echo ""
            echo "üß™ Testing Docker image..."
            CONTAINER_NAME="test-container-${{ github.sha }}"

            docker run --rm -d \
              --name "$CONTAINER_NAME" \
              ${{ steps.docker_build.outputs.image_tag }}

            # Wait for container to start
            echo "‚è≥ Waiting for container to start..."
            sleep 10

            # Check if container is running
            if docker ps --filter "name=$CONTAINER_NAME" --format "{{.Names}}" | grep -q "$CONTAINER_NAME"; then
              echo "‚úÖ Container is running"

              # Test if nginx is responding
              if docker exec "$CONTAINER_NAME" curl -f http://localhost/ >/dev/null 2>&1; then
                echo "‚úÖ Nginx is responding"

                # Check for environment variables in the built JS files
                echo "üîç Checking for embedded environment variables..."
                if docker exec "$CONTAINER_NAME" find /usr/share/nginx/html -name "*.js" -exec grep -l "VITE_API_URL\|VITE_APP_ENV" {} \; | head -1; then
                  echo "‚úÖ Environment variables found in built assets"
                else
                  echo "‚ö†Ô∏è Environment variables not found in assets (may be minified)"
                fi
              else
                echo "‚ö†Ô∏è Nginx may not be fully ready yet"
              fi
            else
              echo "‚ùå Container stopped unexpectedly"
              echo "üìã Container logs:"
              docker logs "$CONTAINER_NAME" || echo "Could not get logs"
              exit 1
            fi

            # Stop test container
            docker stop "$CONTAINER_NAME" || true
            echo "‚úÖ BUILD TEST PASSED"
          else
            echo "‚ùå CRITICAL FAILURE: index.html not found in extracted artifacts."
            exit 1
          fi

      - name: Generate Runtime Config (Staging)
        if: matrix.environment == 'staging'
        run: |
          echo "üîß Generating runtime configuration for S3 staging..."

          # Use CloudFront domain for API to avoid mixed content (temporary solution)
          export API_URL="https://d1mrphf40jf3dj.cloudfront.net/api"
          export APP_URL="${{ secrets.VITE_APP_URL_STAGING }}"
          export APP_ENV="staging"
          export APP_NAME="Shrt"
          export APP_VERSION="1.0.0"
          export COMMIT_HASH="${{ github.sha }}"
          export GIT_BRANCH="${{ github.ref_name }}"
          export BUILD_TIME="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          export ENABLE_DEBUG="true"
          export ENABLE_MONITORING="false"
          export SHOW_PERFORMANCE="true"

          echo "üîç Staging values:"
          echo "  API_URL: '$API_URL'"
          echo "  APP_URL: '$APP_URL'"
          echo "  APP_ENV: '$APP_ENV'"
          echo "  APP_NAME: '$APP_NAME'"
          echo "  APP_VERSION: '$APP_VERSION'"
          echo "  BUILD_TIME: '$BUILD_TIME'"

          # Generate config.js with all secrets
          cat > dist/config.js << EOF
          // Auto-generated configuration for S3 deployment (staging)
          window.__RUNTIME_CONFIG__ = {
            "VITE_API_URL": "$API_URL",
            "VITE_API_BASE_URL": "/api",
            "VITE_APP_URL": "$APP_URL",
            "VITE_APP_ENV": "$APP_ENV",
            "VITE_APP_NAME": "$APP_NAME",
            "VITE_APP_VERSION": "$APP_VERSION",
            "VITE_COMMIT_HASH": "$COMMIT_HASH",
            "VITE_COMMIT_MESSAGE": "Deploy from $GIT_BRANCH",
            "VITE_GIT_BRANCH": "$GIT_BRANCH",
            "VITE_BUILD_TIME": "$BUILD_TIME",
            "VITE_REQUEST_TIMEOUT": "5000",
            "VITE_ENABLE_DEBUG": "$ENABLE_DEBUG",
            "VITE_ENABLE_MONITORING": "$ENABLE_MONITORING",
            "VITE_SHOW_PERFORMANCE_METRICS": "$SHOW_PERFORMANCE"
          };

          // Make it available globally
          if (typeof window !== 'undefined') {
            window.getEnvVar = function(key, fallback = '') {
              return window.__RUNTIME_CONFIG__[key] || fallback;
            };
          }

          console.log('üîß Runtime config loaded:', window.__RUNTIME_CONFIG__);
          EOF

          echo "‚úÖ Generated config.js for S3 staging deployment"
          echo "üìã Config contents:"
          cat dist/config.js

      - name: Generate Runtime Config (Production)
        if: matrix.environment == 'production'
        run: |
          echo "üîß Generating runtime configuration for S3 production..."

          # Use CloudFront domain for API to avoid mixed content (temporary solution)
          export API_URL="https://d3dcezd6ji3gto.cloudfront.net/api"
          export APP_URL="${{ secrets.VITE_APP_URL_PRODUCTION }}"
          export APP_ENV="production"
          export APP_NAME="Shrt"
          export APP_VERSION="1.0.0"
          export COMMIT_HASH="${{ github.sha }}"
          export GIT_BRANCH="${{ github.ref_name }}"
          export BUILD_TIME="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          export ENABLE_DEBUG="false"
          export ENABLE_MONITORING="true"
          export SHOW_PERFORMANCE="false"

          echo "üîç Production values:"
          echo "  API_URL: '$API_URL'"
          echo "  APP_URL: '$APP_URL'"
          echo "  APP_ENV: '$APP_ENV'"
          echo "  APP_NAME: '$APP_NAME'"
          echo "  APP_VERSION: '$APP_VERSION'"
          echo "  BUILD_TIME: '$BUILD_TIME'"

          # Generate config.js with all secrets
          cat > dist/config.js << EOF
          // Auto-generated configuration for S3 deployment (production)
          window.__RUNTIME_CONFIG__ = {
            "VITE_API_URL": "$API_URL",
            "VITE_API_BASE_URL": "/api",
            "VITE_APP_URL": "$APP_URL",
            "VITE_APP_ENV": "$APP_ENV",
            "VITE_APP_NAME": "$APP_NAME",
            "VITE_APP_VERSION": "$APP_VERSION",
            "VITE_COMMIT_HASH": "$COMMIT_HASH",
            "VITE_COMMIT_MESSAGE": "Deploy from $GIT_BRANCH",
            "VITE_GIT_BRANCH": "$GIT_BRANCH",
            "VITE_BUILD_TIME": "$BUILD_TIME",
            "VITE_REQUEST_TIMEOUT": "5000",
            "VITE_ENABLE_DEBUG": "$ENABLE_DEBUG",
            "VITE_ENABLE_MONITORING": "$ENABLE_MONITORING",
            "VITE_SHOW_PERFORMANCE_METRICS": "$SHOW_PERFORMANCE"
          };

          // Make it available globally
          if (typeof window !== 'undefined') {
            window.getEnvVar = function(key, fallback = '') {
              return window.__RUNTIME_CONFIG__[key] || fallback;
            };
          }

          console.log('üîß Runtime config loaded:', window.__RUNTIME_CONFIG__);
          EOF

          echo "‚úÖ Generated config.js for S3 production deployment"
          echo "üìã Config contents:"
          cat dist/config.js

      - name: Optimize Build Output
        run: |
          # Comprimir archivos est√°ticos para mejorar performance
          find dist -name "*.js" -o -name "*.css" -o -name "*.html" -o -name "*.json" | \
          while read file; do
            gzip -c "$file" > "${file}.gz"
            echo "Compressed: $file -> ${file}.gz"
          done

          # Generar manifest de archivos para cache busting
          find dist -type f -exec md5sum {} \; > dist/manifest.txt

      - name: Run Bundle Analysis
        if: matrix.environment == 'production'
        run: |
          echo "üìä Bundle analysis using existing Docker build artifacts..."
          ls -la dist/
          echo "‚úÖ Bundle analysis completed - artifacts preserved from Docker build"

      - name: Upload Build Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-${{ matrix.environment }}-${{ github.sha }}
          path: dist/
          retention-days: 7
          compression-level: 9

  # Job para deploy a staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [quality-checks, build]
    if: (github.ref == 'refs/heads/develop' && github.event_name == 'push') || (github.event_name == 'workflow_dispatch' && inputs.environment == 'staging')
    environment:
      name: staging
      url: https://d1mrphf40jf3dj.cloudfront.net

    steps:
      - name: Download Build Artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-staging-${{ github.sha }}
          path: dist

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Ensure S3 Bucket Exists
        run: |
          if ! aws s3api head-bucket --bucket ${{ env.S3_BUCKET_STAGING }} 2>/dev/null; then
            echo "Creating S3 bucket: ${{ env.S3_BUCKET_STAGING }}"
            aws s3 mb s3://${{ env.S3_BUCKET_STAGING }} --region ${{ env.AWS_REGION }}
          else
            echo "S3 bucket ${{ env.S3_BUCKET_STAGING }} already exists"
          fi

          # Configure bucket ownership and public access
          echo "Configuring bucket ownership..."
          aws s3api put-bucket-ownership-controls \
            --bucket ${{ env.S3_BUCKET_STAGING }} \
            --ownership-controls Rules='[{ObjectOwnership=BucketOwnerEnforced}]' \
            || echo "‚ö†Ô∏è Could not configure bucket ownership"

          # Configure bucket for public website hosting (simple approach)
          echo "Configuring bucket for public website hosting..."
          aws s3api put-public-access-block \
            --bucket ${{ env.S3_BUCKET_STAGING }} \
            --public-access-block-configuration \
            "BlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=false,RestrictPublicBuckets=false" \
            && echo "‚úÖ Bucket configured for public access" || echo "‚ö†Ô∏è Could not configure bucket access"

          sleep 3

      - name: Configure S3 Bucket for Website Hosting
        run: |
          echo "üîí Configuring S3 bucket for website hosting..."

          # Configure S3 website hosting
          echo "üåê Configuring S3 website hosting..."
          aws s3api put-bucket-website --bucket ${{ env.S3_BUCKET_STAGING }} \
            --website-configuration '{
              "IndexDocument": {"Suffix": "index.html"},
              "ErrorDocument": {"Key": "index.html"}
            }' && echo "‚úÖ Website hosting configured" || echo "‚ö†Ô∏è Could not configure website hosting"

          # Set bucket policy to allow public read access
          echo "üìù Setting bucket policy to allow public read access..."
          aws s3api put-bucket-policy --bucket ${{ env.S3_BUCKET_STAGING }} \
            --policy "{
              \"Version\": \"2012-10-17\",
              \"Statement\": [{
                \"Sid\": \"PublicReadGetObject\",
                \"Effect\": \"Allow\",
                \"Principal\": \"*\",
                \"Action\": \"s3:GetObject\",
                \"Resource\": \"arn:aws:s3:::${{ env.S3_BUCKET_STAGING }}/*\"
              }]
            }" && echo "‚úÖ Bucket policy applied successfully" || echo "‚ö†Ô∏è Could not set bucket policy"

      - name: Ensure CloudFront Distribution Exists
        run: |
          echo "üîç Checking CloudFront distribution for staging..."

          # Use configured CloudFront distribution ID for staging
          DISTRIBUTION_ID="${{ secrets.CLOUDFRONT_DISTRIBUTION_STAGING }}"
          if [ -n "$DISTRIBUTION_ID" ]; then
            echo "Using configured staging distribution ID: $DISTRIBUTION_ID"
          else
            echo "No distribution ID found in secrets, checking for existing distributions..."

            # Look for existing distributions with our S3 website origin
            WEBSITE_ENDPOINT="${{ env.S3_BUCKET_STAGING }}.s3-website-${{ env.AWS_REGION }}.amazonaws.com"
            EXISTING_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?Origins.Items[?DomainName=='$WEBSITE_ENDPOINT']].Id" --output text 2>/dev/null || echo "")

            if [ -n "$EXISTING_ID" ] && [ "$EXISTING_ID" != "None" ]; then
              echo "Found existing distribution: $EXISTING_ID"
              DISTRIBUTION_ID="$EXISTING_ID"
            else
              echo "Creating new CloudFront distribution for staging..."

              # Create simple CloudFront distribution using S3 website endpoint
              DISTRIBUTION_ID=$(aws cloudfront create-distribution \
                --distribution-config '{
                  "CallerReference": "shrt-staging-'$(date +%Y%m%d-%H%M%S)'",
                  "Comment": "Shrt Staging Frontend CDN Distribution",
                  "DefaultRootObject": "index.html",
                  "Origins": {
                    "Quantity": 2,
                    "Items": [{
                      "Id": "S3-Website-'${{ env.S3_BUCKET_STAGING }}'",
                      "DomainName": "'$WEBSITE_ENDPOINT'",
                      "CustomOriginConfig": {
                        "HTTPPort": 80,
                        "HTTPSPort": 443,
                        "OriginProtocolPolicy": "http-only"
                      }
                    }, {
                      "Id": "API-Origin",
                      "DomainName": "shrt-staging-alb-xxxxxx.us-east-1.elb.amazonaws.com",
                      "CustomOriginConfig": {
                        "HTTPPort": 80,
                        "HTTPSPort": 443,
                        "OriginProtocolPolicy": "http-only"
                      }
                    }]
                  },
                  "DefaultCacheBehavior": {
                    "TargetOriginId": "S3-Website-'${{ env.S3_BUCKET_STAGING }}'",
                    "ViewerProtocolPolicy": "redirect-to-https",
                    "TrustedSigners": {
                      "Enabled": false,
                      "Quantity": 0
                    },
                    "ForwardedValues": {
                      "QueryString": false,
                      "Cookies": {
                        "Forward": "none"
                      }
                    },
                    "MinTTL": 0,
                    "DefaultTTL": 86400,
                    "MaxTTL": 31536000,
                    "Compress": true
                  },
                  "CacheBehaviors": {
                    "Quantity": 1,
                    "Items": [{
                      "PathPattern": "/api/*",
                      "TargetOriginId": "API-Origin",
                      "ViewerProtocolPolicy": "redirect-to-https",
                      "TrustedSigners": {
                        "Enabled": false,
                        "Quantity": 0
                      },
                      "ForwardedValues": {
                        "QueryString": true,
                        "Cookies": {
                          "Forward": "all"
                        },
                        "Headers": ["*"]
                      },
                      "MinTTL": 0,
                      "DefaultTTL": 0,
                      "MaxTTL": 0,
                      "Compress": false
                    }]
                  },
                  "CustomErrorResponses": {
                    "Quantity": 1,
                    "Items": [{
                      "ErrorCode": 404,
                      "ResponsePagePath": "/index.html",
                      "ResponseCode": "200",
                      "ErrorCachingMinTTL": 300
                    }]
                  },
                  "Enabled": true,
                  "PriceClass": "PriceClass_100"
                }' \
                --query 'Distribution.Id' --output text)

              echo "‚úÖ Created new CloudFront distribution: $DISTRIBUTION_ID"
              echo "‚è≥ Distribution is being deployed, this may take several minutes..."
            fi
          fi

          echo "CLOUDFRONT_DISTRIBUTION_ID=$DISTRIBUTION_ID" >> $GITHUB_ENV

          # Force refresh bucket policy to ensure it's applied
          echo "üîÑ Ensuring bucket policy is properly applied..."
          aws s3api put-bucket-policy --bucket ${{ env.S3_BUCKET_STAGING }} \
            --policy "{
              \"Version\": \"2012-10-17\",
              \"Statement\": [{
                \"Sid\": \"PublicReadGetObject\",
                \"Effect\": \"Allow\",
                \"Principal\": \"*\",
                \"Action\": \"s3:GetObject\",
                \"Resource\": \"arn:aws:s3:::${{ env.S3_BUCKET_STAGING }}/*\"
              }]
            }" && echo "‚úÖ Bucket policy reapplied successfully" || echo "‚ö†Ô∏è Could not reapply bucket policy"


      - name: Deploy to S3 Staging
        run: |
          echo "üöÄ Deploying to staging environment..."

          # Deploy all files with proper cache settings
          echo "üöÄ Deploying to S3..."

          # Sync files with proper content types
          echo "üìÅ Uploading files with correct MIME types..."

          # Upload HTML files
          find dist/ -name "*.html" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_STAGING }}/$key" \
              --content-type "text/html" \
              --cache-control "public, max-age=0, must-revalidate"
          done

          # Upload JavaScript files
          find dist/ -name "*.js" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_STAGING }}/$key" \
              --content-type "application/javascript" \
              --cache-control "public, max-age=31536000, immutable"
          done

          # Upload CSS files
          find dist/ -name "*.css" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_STAGING }}/$key" \
              --content-type "text/css" \
              --cache-control "public, max-age=31536000, immutable"
          done

          # Upload config.js with correct MIME type
          if [ -f "dist/config.js" ]; then
            aws s3 cp "dist/config.js" "s3://${{ env.S3_BUCKET_STAGING }}/config.js" \
              --content-type "application/javascript" \
              --cache-control "public, max-age=0, must-revalidate"
          fi

          # Upload other assets with appropriate content types
          find dist/ -name "*.json" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_STAGING }}/$key" \
              --content-type "application/json" \
              --cache-control "public, max-age=86400"
          done

          find dist/ -name "*.svg" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_STAGING }}/$key" \
              --content-type "image/svg+xml" \
              --cache-control "public, max-age=31536000, immutable"
          done

          find dist/ -name "*.ico" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_STAGING }}/$key" \
              --content-type "image/x-icon" \
              --cache-control "public, max-age=31536000, immutable"
          done

          # Sync any remaining files (like images) with default settings
          aws s3 sync dist/ s3://${{ env.S3_BUCKET_STAGING }}/ --delete \
            --exclude "*.html" --exclude "*.js" --exclude "*.css" \
            --exclude "*.json" --exclude "*.svg" --exclude "*.ico"

          echo "‚úÖ Staging deployment completed with proper MIME types" \
            || echo "‚ö†Ô∏è No SEO files found to configure"

          echo "‚úÖ All files deployed successfully"

          # Configurar headers espec√≠ficos para archivos comprimidos
          for file in $(find dist -name "*.gz" -type f); do
            key="${file#dist/}"
            key="${key%.gz}"

            content_type=""
            case "$key" in
              *.js) content_type="application/javascript" ;;
              *.css) content_type="text/css" ;;
              *.html) content_type="text/html" ;;
              *.json) content_type="application/json" ;;
            esac

            if [ ! -z "$content_type" ]; then
              aws s3 cp "$file" "s3://${{ env.S3_BUCKET_STAGING }}/$key" \
                --content-encoding gzip \
                --content-type "$content_type" \
                --cache-control "public, max-age=31536000, immutable"
            fi
          done

      - name: Invalidate CloudFront Cache
        run: |
          if [ -n "$CLOUDFRONT_DISTRIBUTION_ID" ]; then
            echo "üîÑ Invalidating CloudFront cache for distribution: $CLOUDFRONT_DISTRIBUTION_ID"
            INVALIDATION_ID=$(aws cloudfront create-invalidation \
              --distribution-id $CLOUDFRONT_DISTRIBUTION_ID \
              --paths "/*" \
              --query 'Invalidation.Id' \
              --output text)
            echo "‚úÖ Cache invalidation created: $INVALIDATION_ID"

            # Get the CloudFront domain name for verification
            DOMAIN_NAME=$(aws cloudfront get-distribution \
              --id $CLOUDFRONT_DISTRIBUTION_ID \
              --query 'Distribution.DomainName' \
              --output text)
            echo "üåê CloudFront URL: https://$DOMAIN_NAME"

            # Save distribution info for the environment
            echo "CLOUDFRONT_DOMAIN_STAGING=$DOMAIN_NAME" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è CloudFront distribution ID not found, skipping cache invalidation"
          fi

      - name: Health Check
        run: |
          echo "üîç Verificando deployment..."

          if [ -n "$CLOUDFRONT_DISTRIBUTION_ID" ]; then
            # Get the actual CloudFront domain name
            DOMAIN_NAME=$(aws cloudfront get-distribution \
              --id $CLOUDFRONT_DISTRIBUTION_ID \
              --query 'Distribution.DomainName' \
              --output text)
            CLOUDFRONT_URL="https://$DOMAIN_NAME"
            echo "Using CloudFront URL: $CLOUDFRONT_URL"

            for i in {1..15}; do
              echo "Intento $i/15: Verificando CloudFront deployment..."

              RESPONSE=$(curl -s -w "HTTP_CODE:%{http_code}" "$CLOUDFRONT_URL/" 2>/dev/null || echo "CURL_FAILED")
              if echo "$RESPONSE" | grep -q "HTTP_CODE:200"; then
                if echo "$RESPONSE" | grep -q -i "shrt\|react\|vite" 2>/dev/null; then
                  echo "‚úÖ Staging deployment verificado exitosamente (CloudFront)"
                  exit 0
                else
                  echo "  HTTP 200 pero contenido no reconocido, continuando..."
                fi
              else
                echo "  Response: $(echo "$RESPONSE" | tail -1)"
              fi

              if [ $i -eq 15 ]; then
                echo "‚ö†Ô∏è Verificaci√≥n CloudFront fall√≥ despu√©s de 15 intentos"
              else
                sleep 20
              fi
            done
          fi

          # Fallback: verificar que los archivos est√©n en S3
          echo "üîç Verificando archivos en S3..."
          if aws s3api head-object --bucket ${{ env.S3_BUCKET_STAGING }} --key index.html >/dev/null 2>&1; then
            echo "‚úÖ index.html encontrado en S3 - deployment completado"
          else
            echo "‚ùå index.html no encontrado en S3"
            exit 1
          fi

  # Job para deploy a producci√≥n
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [quality-checks, build]
    if: (github.ref == 'refs/heads/main' && github.event_name == 'push') || (github.event_name == 'workflow_dispatch' && inputs.environment == 'production')
    environment:
      name: production
      url: https://d3dcezd6ji3gto.cloudfront.net

    steps:
      - name: Download Build Artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-production-${{ github.sha }}
          path: dist

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Ensure S3 Bucket Exists
        run: |
          if ! aws s3api head-bucket --bucket ${{ env.S3_BUCKET_PRODUCTION }} 2>/dev/null; then
            echo "Creating S3 bucket: ${{ env.S3_BUCKET_PRODUCTION }}"
            aws s3 mb s3://${{ env.S3_BUCKET_PRODUCTION }} --region ${{ env.AWS_REGION }}
          else
            echo "S3 bucket ${{ env.S3_BUCKET_PRODUCTION }} already exists"
          fi

          # Configure bucket ownership and public access
          echo "Configuring bucket ownership..."
          aws s3api put-bucket-ownership-controls \
            --bucket ${{ env.S3_BUCKET_PRODUCTION }} \
            --ownership-controls Rules='[{ObjectOwnership=BucketOwnerEnforced}]' \
            || echo "‚ö†Ô∏è Could not configure bucket ownership"

          # Configure bucket for public website hosting (simple approach)
          echo "Configuring bucket for public website hosting..."
          aws s3api put-public-access-block \
            --bucket ${{ env.S3_BUCKET_PRODUCTION }} \
            --public-access-block-configuration \
            "BlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=false,RestrictPublicBuckets=false" \
            && echo "‚úÖ Bucket configured for public access" || echo "‚ö†Ô∏è Could not configure bucket access"

          sleep 3

      - name: Configure S3 Bucket for Website Hosting
        run: |
          echo "üîí Configuring S3 bucket for website hosting..."

          # Configure S3 website hosting
          echo "üåê Configuring S3 website hosting..."
          aws s3api put-bucket-website --bucket ${{ env.S3_BUCKET_PRODUCTION }} \
            --website-configuration '{
              "IndexDocument": {"Suffix": "index.html"},
              "ErrorDocument": {"Key": "index.html"}
            }' && echo "‚úÖ Website hosting configured" || echo "‚ö†Ô∏è Could not configure website hosting"

          # Set bucket policy to allow public read access
          echo "üìù Setting bucket policy to allow public read access..."
          aws s3api put-bucket-policy --bucket ${{ env.S3_BUCKET_PRODUCTION }} \
            --policy "{
              \"Version\": \"2012-10-17\",
              \"Statement\": [{
                \"Sid\": \"PublicReadGetObject\",
                \"Effect\": \"Allow\",
                \"Principal\": \"*\",
                \"Action\": \"s3:GetObject\",
                \"Resource\": \"arn:aws:s3:::${{ env.S3_BUCKET_PRODUCTION }}/*\"
              }]
            }" && echo "‚úÖ Bucket policy applied successfully" || echo "‚ö†Ô∏è Could not set bucket policy"

      - name: Ensure CloudFront Distribution Exists
        run: |
          echo "üîç Checking CloudFront distribution for production..."

          # Use configured CloudFront distribution ID for production
          DISTRIBUTION_ID="${{ secrets.CLOUDFRONT_DISTRIBUTION_PRODUCTION }}"
          if [ -n "$DISTRIBUTION_ID" ]; then
            echo "Using configured production distribution ID: $DISTRIBUTION_ID"
          else
            echo "No distribution ID found in secrets, checking for existing distributions..."

            # Look for existing distributions with our S3 website origin
            WEBSITE_ENDPOINT="${{ env.S3_BUCKET_PRODUCTION }}.s3-website-${{ env.AWS_REGION }}.amazonaws.com"
            EXISTING_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?Origins.Items[?DomainName=='$WEBSITE_ENDPOINT']].Id" --output text 2>/dev/null || echo "")

            if [ -n "$EXISTING_ID" ] && [ "$EXISTING_ID" != "None" ]; then
              echo "Found existing distribution: $EXISTING_ID"
              DISTRIBUTION_ID="$EXISTING_ID"
            else
              echo "Creating new CloudFront distribution for production..."

              # Create simple CloudFront distribution using S3 website endpoint
              DISTRIBUTION_ID=$(aws cloudfront create-distribution \
                --distribution-config '{
                  "CallerReference": "shrt-production-'$(date +%Y%m%d-%H%M%S)'",
                  "Comment": "Shrt Production Frontend CDN Distribution",
                  "DefaultRootObject": "index.html",
                  "Origins": {
                    "Quantity": 2,
                    "Items": [{
                      "Id": "S3-Website-'${{ env.S3_BUCKET_PRODUCTION }}'",
                      "DomainName": "'$WEBSITE_ENDPOINT'",
                      "CustomOriginConfig": {
                        "HTTPPort": 80,
                        "HTTPSPort": 443,
                        "OriginProtocolPolicy": "http-only"
                      }
                    }, {
                      "Id": "API-Origin",
                      "DomainName": "shrt-production-alb-132772302.us-east-1.elb.amazonaws.com",
                      "CustomOriginConfig": {
                        "HTTPPort": 80,
                        "HTTPSPort": 443,
                        "OriginProtocolPolicy": "http-only"
                      }
                    }]
                  },
                  "DefaultCacheBehavior": {
                    "TargetOriginId": "S3-Website-'${{ env.S3_BUCKET_PRODUCTION }}'",
                    "ViewerProtocolPolicy": "redirect-to-https",
                    "TrustedSigners": {
                      "Enabled": false,
                      "Quantity": 0
                    },
                    "ForwardedValues": {
                      "QueryString": false,
                      "Cookies": {
                        "Forward": "none"
                      }
                    },
                    "MinTTL": 0,
                    "DefaultTTL": 86400,
                    "MaxTTL": 31536000,
                    "Compress": true
                  },
                  "CacheBehaviors": {
                    "Quantity": 3,
                    "Items": [{
                      "PathPattern": "/api/*",
                      "TargetOriginId": "API-Origin",
                      "ViewerProtocolPolicy": "redirect-to-https",
                      "TrustedSigners": {
                        "Enabled": false,
                        "Quantity": 0
                      },
                      "ForwardedValues": {
                        "QueryString": true,
                        "Cookies": {
                          "Forward": "all"
                        },
                        "Headers": ["*"]
                      },
                      "MinTTL": 0,
                      "DefaultTTL": 0,
                      "MaxTTL": 0,
                      "Compress": false
                    }, {
                      "PathPattern": "/assets/*",
                      "TargetOriginId": "S3-Website-'${{ env.S3_BUCKET_PRODUCTION }}'",
                      "ViewerProtocolPolicy": "redirect-to-https",
                      "TrustedSigners": {
                        "Enabled": false,
                        "Quantity": 0
                      },
                      "ForwardedValues": {
                        "QueryString": false,
                        "Cookies": {
                          "Forward": "none"
                        }
                      },
                      "MinTTL": 31536000,
                      "DefaultTTL": 31536000,
                      "MaxTTL": 31536000,
                      "Compress": true
                    }, {
                      "PathPattern": "*.js",
                      "TargetOriginId": "S3-Website-'${{ env.S3_BUCKET_PRODUCTION }}'",
                      "ViewerProtocolPolicy": "redirect-to-https",
                      "TrustedSigners": {
                        "Enabled": false,
                        "Quantity": 0
                      },
                      "ForwardedValues": {
                        "QueryString": false,
                        "Cookies": {
                          "Forward": "none"
                        }
                      },
                      "MinTTL": 31536000,
                      "DefaultTTL": 31536000,
                      "MaxTTL": 31536000,
                      "Compress": true
                    }]
                  },
                  "CustomErrorResponses": {
                    "Quantity": 1,
                    "Items": [{
                      "ErrorCode": 404,
                      "ResponsePagePath": "/index.html",
                      "ResponseCode": "200",
                      "ErrorCachingMinTTL": 300
                    }]
                  },
                  "Enabled": true,
                  "PriceClass": "PriceClass_All"
                }' \
                --query 'Distribution.Id' --output text)

              echo "‚úÖ Created new CloudFront distribution: $DISTRIBUTION_ID"
              echo "‚è≥ Distribution is being deployed, this may take several minutes..."
            fi
          fi

          echo "CLOUDFRONT_DISTRIBUTION_ID=$DISTRIBUTION_ID" >> $GITHUB_ENV

          # Force refresh bucket policy to ensure it's applied
          echo "üîÑ Ensuring bucket policy is properly applied..."
          aws s3api put-bucket-policy --bucket ${{ env.S3_BUCKET_PRODUCTION }} \
            --policy "{
              \"Version\": \"2012-10-17\",
              \"Statement\": [{
                \"Sid\": \"PublicReadGetObject\",
                \"Effect\": \"Allow\",
                \"Principal\": \"*\",
                \"Action\": \"s3:GetObject\",
                \"Resource\": \"arn:aws:s3:::${{ env.S3_BUCKET_PRODUCTION }}/*\"
              }]
            }" && echo "‚úÖ Bucket policy reapplied successfully" || echo "‚ö†Ô∏è Could not reapply bucket policy"

      - name: Create Backup
        run: |
          echo "üì¶ Creando backup del deployment anterior..."
          if aws s3api head-bucket --bucket ${{ env.S3_BUCKET_PRODUCTION }} 2>/dev/null; then
            if aws s3 ls s3://${{ env.S3_BUCKET_PRODUCTION }}/ 2>/dev/null | head -1; then
              BACKUP_NAME="backup-$(date +%Y%m%d-%H%M%S)"
              echo "Creating backup: $BACKUP_NAME"
              aws s3 sync s3://${{ env.S3_BUCKET_PRODUCTION }}/ s3://${{ env.S3_BUCKET_PRODUCTION }}-backups/$BACKUP_NAME/ || echo "‚ö†Ô∏è Could not create backup"
            else
              echo "Bucket is empty, no backup needed"
            fi
          else
            echo "Bucket does not exist yet, no backup needed"
          fi


      - name: Deploy to S3 Production
        run: |
          echo "üöÄ Deploying to production environment..."

          # Deploy all files with proper MIME types and cache settings
          echo "üöÄ Deploying to S3..."
          echo "üìÅ Uploading files with correct MIME types..."

          # Upload HTML files
          find dist/ -name "*.html" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "text/html" \
              --cache-control "public, max-age=0, must-revalidate"
          done

          # Upload JavaScript files
          find dist/ -name "*.js" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "application/javascript" \
              --cache-control "public, max-age=31536000, immutable"
          done

          # Upload CSS files
          find dist/ -name "*.css" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "text/css" \
              --cache-control "public, max-age=31536000, immutable"
          done

          # Upload config.js with correct MIME type
          if [ -f "dist/config.js" ]; then
            aws s3 cp "dist/config.js" "s3://${{ env.S3_BUCKET_PRODUCTION }}/config.js" \
              --content-type "application/javascript" \
              --cache-control "public, max-age=0, must-revalidate"
          fi

          # Upload JSON files (manifest, etc.)
          find dist/ -name "*.json" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "application/json" \
              --cache-control "public, max-age=86400"
          done

          # Upload SVG files
          find dist/ -name "*.svg" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "image/svg+xml" \
              --cache-control "public, max-age=31536000, immutable"
          done

          # Upload ICO files
          find dist/ -name "*.ico" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "image/x-icon" \
              --cache-control "public, max-age=31536000, immutable"
          done

          # Upload TXT files (robots.txt, sitemap.xml)
          find dist/ -name "*.txt" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "text/plain" \
              --cache-control "public, max-age=86400"
          done

          find dist/ -name "*.xml" -type f | while read file; do
            key="${file#dist/}"
            aws s3 cp "$file" "s3://${{ env.S3_BUCKET_PRODUCTION }}/$key" \
              --content-type "application/xml" \
              --cache-control "public, max-age=86400"
          done

          # Sync any remaining files (like images) with default settings
          aws s3 sync dist/ s3://${{ env.S3_BUCKET_PRODUCTION }}/ --delete \
            --exclude "*.html" --exclude "*.js" --exclude "*.css" \
            --exclude "*.json" --exclude "*.svg" --exclude "*.ico" \
            --exclude "*.txt" --exclude "*.xml"

          echo "‚úÖ Production deployment completed with proper MIME types" \
            || echo "‚ö†Ô∏è No SEO files found to configure"

          echo "‚úÖ All files deployed successfully"

      - name: Invalidate CloudFront Cache
        run: |
          if [ -n "$CLOUDFRONT_DISTRIBUTION_ID" ]; then
            echo "üîÑ Invalidating CloudFront cache for distribution: $CLOUDFRONT_DISTRIBUTION_ID"
            INVALIDATION_ID=$(aws cloudfront create-invalidation \
              --distribution-id $CLOUDFRONT_DISTRIBUTION_ID \
              --paths "/*" \
              --query 'Invalidation.Id' \
              --output text)

            echo "üîÑ CloudFront invalidation creada: $INVALIDATION_ID"

            # Get the CloudFront domain name for verification
            DOMAIN_NAME=$(aws cloudfront get-distribution \
              --id $CLOUDFRONT_DISTRIBUTION_ID \
              --query 'Distribution.DomainName' \
              --output text)
            echo "üåê CloudFront URL: https://$DOMAIN_NAME"

            # Save distribution info for the environment
            echo "CLOUDFRONT_DOMAIN_PRODUCTION=$DOMAIN_NAME" >> $GITHUB_ENV

            # Esperar que se complete la invalidaci√≥n (solo para producci√≥n)
            echo "‚è≥ Esperando que se complete la invalidaci√≥n..."
            aws cloudfront wait invalidation-completed \
              --distribution-id $CLOUDFRONT_DISTRIBUTION_ID \
              --id $INVALIDATION_ID

            echo "‚úÖ Invalidaci√≥n de CloudFront completada"
          else
            echo "‚ö†Ô∏è CloudFront distribution ID not found, skipping cache invalidation"
          fi

      - name: Health Check & Smoke Tests
        run: |
          echo "üîç Ejecutando health checks de producci√≥n..."

          if [ -n "$CLOUDFRONT_DISTRIBUTION_ID" ]; then
            # Get the actual CloudFront domain name
            DOMAIN_NAME=$(aws cloudfront get-distribution \
              --id $CLOUDFRONT_DISTRIBUTION_ID \
              --query 'Distribution.DomainName' \
              --output text)
            CLOUDFRONT_URL="https://$DOMAIN_NAME"
            echo "Using CloudFront URL: $CLOUDFRONT_URL"

            # Health check b√°sico con debug info
            HEALTH_CHECK_PASSED=false
            for i in {1..20}; do
              echo "Intento $i/20: Verificando CloudFront deployment..."

              RESPONSE=$(curl -s -w "HTTP_CODE:%{http_code}" "$CLOUDFRONT_URL/" 2>/dev/null || echo "CURL_FAILED")
              if echo "$RESPONSE" | grep -q "HTTP_CODE:200"; then
                if echo "$RESPONSE" | grep -q -i "shrt\|react\|vite" 2>/dev/null; then
                  echo "‚úÖ Production deployment verificado exitosamente (CloudFront)"
                  HEALTH_CHECK_PASSED=true
                  break
                else
                  echo "  HTTP 200 pero contenido no reconocido, continuando..."
                fi
              else
                echo "  Response: $(echo "$RESPONSE" | tail -1)"
              fi

              if [ $i -eq 20 ]; then
                echo "‚ö†Ô∏è Verificaci√≥n CloudFront fall√≥ despu√©s de 20 intentos"
              else
                sleep 30
              fi
            done

            if [ "$HEALTH_CHECK_PASSED" = false ]; then
              echo "‚ö†Ô∏è Health check CloudFront fall√≥, pero deployment puede estar disponible"

              # List S3 contents for debugging
              echo "üìã Contenido del bucket S3:"
              aws s3 ls s3://${{ env.S3_BUCKET_PRODUCTION }}/ --recursive | head -10 || echo "No se pudo listar el contenido"
            else
              echo "‚úÖ Production deployment verificado exitosamente"
            fi
          fi

          # Simple smoke test - just verify deployment succeeded
          echo "üîç Verificaci√≥n final de archivos..."
          if aws s3api head-object --bucket ${{ env.S3_BUCKET_PRODUCTION }} --key index.html >/dev/null 2>&1; then
            echo "‚úÖ index.html encontrado en S3 - deployment completado"
          else
            echo "‚ùå index.html no encontrado en S3"
            exit 1
          fi

  # Job para an√°lisis de performance post-deployment
  performance-audit:
    name: Performance Audit
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production]
    if: always() && (needs.deploy-staging.result == 'success' || needs.deploy-production.result == 'success')

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.12.x

      - name: Run Lighthouse CI - Staging
        if: needs.deploy-staging.result == 'success'
        run: |
          lhci autorun \
            --collect.url=https://d1mrphf40jf3dj.cloudfront.net \
            --upload.target=temporary-public-storage \
            --collect.numberOfRuns=3
        continue-on-error: true

      - name: Run Lighthouse CI - Production
        if: needs.deploy-production.result == 'success'
        run: |
          lhci autorun \
            --collect.url=https://d3dcezd6ji3gto.cloudfront.net \
            --upload.target=temporary-public-storage \
            --collect.numberOfRuns=5
        continue-on-error: true

  # Job para notificaciones
  notify:
    name: Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production, performance-audit]
    if: always()

    steps:
      - name: Prepare Notification
        id: prepare
        run: |
          STATUS="‚úÖ √âxito"
          if [[ "${{ needs.deploy-staging.result }}" == "failure" ]] || [[ "${{ needs.deploy-production.result }}" == "failure" ]]; then
            STATUS="‚ùå Fall√≥"
          fi

          ENVIRONMENT="ninguno"
          if [[ "${{ needs.deploy-staging.result }}" == "success" ]]; then
            ENVIRONMENT="staging"
          fi
          if [[ "${{ needs.deploy-production.result }}" == "success" ]]; then
            if [ "$ENVIRONMENT" == "staging" ]; then
              ENVIRONMENT="staging y production"
            else
              ENVIRONMENT="production"
            fi
          fi

          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT

      - name: Notify Status
        run: |
          echo "üöÄ Frontend Deployment Status: ${{ steps.prepare.outputs.status }}"
          echo "üåç Entornos desplegados: ${{ steps.prepare.outputs.environment }}"
          echo "üìä Performance audit: ${{ needs.performance-audit.result }}"
          echo "üîó Commit: ${{ github.sha }}"
          echo "üë§ Actor: ${{ github.actor }}"

          # Aqu√≠ se podr√≠a integrar con Slack, Discord, Teams, etc.
          # curl -X POST -H 'Content-type: application/json' \
          #   --data "{\"text\":\"Frontend deployment completed: ${{ steps.prepare.outputs.status }}\"}" \
          #   ${{ secrets.SLACK_WEBHOOK_URL }}
